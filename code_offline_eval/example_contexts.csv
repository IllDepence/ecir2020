100047375␞␞1208.3719␞As demonstrated in MAINCIT , this allows for trivial, yet effective parallelization of the optimization process: simply perform FORMULA independent runs of the optimization method in parallel and select the result of the run with the lowest cross-validation error to return.Other, more sophisticated methods for the parallelization of Bayesian optimization exist CIT , but to date, there is no empirical evidence that these methods outperform the simple approach we use here when the cost of evaluating hyperparameter configurations varies across the space.␞88740403␟88740403␟512554520␞[[1, ["allow:result", "allow:more sophisticated methods", "allow:bayesian optimization", "allow:date", "allow:run", "allow:parallel", "allow:cost", "allow:optimization method", "allow:space", "allow:trivial yet effective parallelization", "allow:simple approach", "allow:parallelization", "allow:hyperparameter", "allow:independent runs", "allow:optimization process", "allow:hyperparameter configurations", "allow:empirical evidence", "allow:bayesian", "allow:methods", "allow:lowest cross validation error", "allow:optimization", "perform:independent runs", "perform:run", "perform:result", "perform:parallel", "perform:optimization method", "perform:lowest cross validation error", "perform:optimization", "select:lowest cross validation error", "select:run", "select:result", "exist:bayesian optimization", "exist:parallelization", "exist:bayesian", "exist:more sophisticated methods", "be:hyperparameter configurations", "be:cost", "be:empirical evidence", "be:simple approach", "be:hyperparameter", "be:methods", "be:date", "be:space", "outperform:hyperparameter configurations", "outperform:cost", "outperform:simple approach", "outperform:hyperparameter", "outperform:methods", "outperform:space", "use:cost", "use:space", "use:hyperparameter", "use:hyperparameter configurations", "vary:cost", "vary:space", "vary:hyperparameter", "vary:hyperparameter configurations"]]]␟[[1, ["allow:result", "allow:more sophisticated methods", "allow:bayesian optimization", "allow:date", "allow:run", "allow:parallel", "allow:cost", "allow:optimization method", "allow:space", "allow:trivial yet effective parallelization", "allow:simple approach", "allow:parallelization", "allow:hyperparameter", "allow:independent runs", "allow:optimization process", "allow:hyperparameter configurations", "allow:empirical evidence", "allow:bayesian", "allow:methods", "allow:lowest cross validation error", "allow:optimization", "perform:result", "perform:more sophisticated methods", "perform:bayesian optimization", "perform:date", "perform:run", "perform:parallel", "perform:cost", "perform:optimization method", "perform:space", "perform:trivial yet effective parallelization", "perform:simple approach", "perform:parallelization", "perform:hyperparameter", "perform:independent runs", "perform:optimization process", "perform:hyperparameter configurations", "perform:empirical evidence", "perform:bayesian", "perform:methods", "perform:lowest cross validation error", "perform:optimization", "select:result", "select:more sophisticated methods", "select:bayesian optimization", "select:date", "select:run", "select:parallel", "select:cost", "select:optimization method", "select:space", "select:trivial yet effective parallelization", "select:simple approach", "select:parallelization", "select:hyperparameter", "select:independent runs", "select:optimization process", "select:hyperparameter configurations", "select:empirical evidence", "select:bayesian", "select:methods", "select:lowest cross validation error", "select:optimization", "exist:result", "exist:more sophisticated methods", "exist:bayesian optimization", "exist:date", "exist:run", "exist:parallel", "exist:cost", "exist:optimization method", "exist:space", "exist:trivial yet effective parallelization", "exist:simple approach", "exist:parallelization", "exist:hyperparameter", "exist:independent runs", "exist:optimization process", "exist:hyperparameter configurations", "exist:empirical evidence", "exist:bayesian", "exist:methods", "exist:lowest cross validation error", "exist:optimization", "be:result", "be:more sophisticated methods", "be:bayesian optimization", "be:date", "be:run", "be:parallel", "be:cost", "be:optimization method", "be:space", "be:trivial yet effective parallelization", "be:simple approach", "be:parallelization", "be:hyperparameter", "be:independent runs", "be:optimization process", "be:hyperparameter configurations", "be:empirical evidence", "be:bayesian", "be:methods", "be:lowest cross validation error", "be:optimization", "outperform:result", "outperform:more sophisticated methods", "outperform:bayesian optimization", "outperform:date", "outperform:run", "outperform:parallel", "outperform:cost", "outperform:optimization method", "outperform:space", "outperform:trivial yet effective parallelization", "outperform:simple approach", "outperform:parallelization", "outperform:hyperparameter", "outperform:independent runs", "outperform:optimization process", "outperform:hyperparameter configurations", "outperform:empirical evidence", "outperform:bayesian", "outperform:methods", "outperform:lowest cross validation error", "outperform:optimization", "use:result", "use:more sophisticated methods", "use:bayesian optimization", "use:date", "use:run", "use:parallel", "use:cost", "use:optimization method", "use:space", "use:trivial yet effective parallelization", "use:simple approach", "use:parallelization", "use:hyperparameter", "use:independent runs", "use:optimization process", "use:hyperparameter configurations", "use:empirical evidence", "use:bayesian", "use:methods", "use:lowest cross validation error", "use:optimization", "vary:result", "vary:more sophisticated methods", "vary:bayesian optimization", "vary:date", "vary:run", "vary:parallel", "vary:cost", "vary:optimization method", "vary:space", "vary:trivial yet effective parallelization", "vary:simple approach", "vary:parallelization", "vary:hyperparameter", "vary:independent runs", "vary:optimization process", "vary:hyperparameter configurations", "vary:empirical evidence", "vary:bayesian", "vary:methods", "vary:lowest cross validation error", "vary:optimization"]]]␞effective parallelization␟optimization process␟independent runs␟optimization method␟parallel and␟sophisticated methods␟Bayesian optimization␟empirical evidence␟simple approach␟hyperparameter configurations␟trivial␟yet␟FORMULA␟select␟result␟lowest␟error␟date␟use␟cost␟evaluating␝
100047375␞2106411961␟2146409231␟2131241448␞1208.3719␞As demonstrated in CIT , this allows for trivial, yet effective parallelization of the optimization process: simply perform FORMULA independent runs of the optimization method in parallel and select the result of the run with the lowest cross-validation error to return.Other, more sophisticated methods for the parallelization of Bayesian optimization exist MAINCIT , but to date, there is no empirical evidence that these methods outperform the simple approach we use here when the cost of evaluating hyperparameter configurations varies across the space.␞88740403␟88740403␟512554520␞[[1, ["allow:result", "allow:more sophisticated methods", "allow:bayesian optimization", "allow:date", "allow:run", "allow:parallel", "allow:cost", "allow:optimization method", "allow:space", "allow:trivial yet effective parallelization", "allow:simple approach", "allow:parallelization", "allow:hyperparameter", "allow:independent runs", "allow:optimization process", "allow:hyperparameter configurations", "allow:empirical evidence", "allow:bayesian", "allow:methods", "allow:lowest cross validation error", "allow:optimization", "perform:independent runs", "perform:run", "perform:result", "perform:parallel", "perform:optimization method", "perform:lowest cross validation error", "perform:optimization", "select:lowest cross validation error", "select:run", "select:result", "exist:bayesian optimization", "exist:parallelization", "exist:bayesian", "exist:more sophisticated methods", "be:hyperparameter configurations", "be:cost", "be:empirical evidence", "be:simple approach", "be:hyperparameter", "be:methods", "be:date", "be:space", "outperform:hyperparameter configurations", "outperform:cost", "outperform:simple approach", "outperform:hyperparameter", "outperform:methods", "outperform:space", "use:cost", "use:space", "use:hyperparameter", "use:hyperparameter configurations", "vary:cost", "vary:space", "vary:hyperparameter", "vary:hyperparameter configurations"]]]␟[[1, ["allow:result", "allow:more sophisticated methods", "allow:bayesian optimization", "allow:date", "allow:run", "allow:parallel", "allow:cost", "allow:optimization method", "allow:space", "allow:trivial yet effective parallelization", "allow:simple approach", "allow:parallelization", "allow:hyperparameter", "allow:independent runs", "allow:optimization process", "allow:hyperparameter configurations", "allow:empirical evidence", "allow:bayesian", "allow:methods", "allow:lowest cross validation error", "allow:optimization", "perform:result", "perform:more sophisticated methods", "perform:bayesian optimization", "perform:date", "perform:run", "perform:parallel", "perform:cost", "perform:optimization method", "perform:space", "perform:trivial yet effective parallelization", "perform:simple approach", "perform:parallelization", "perform:hyperparameter", "perform:independent runs", "perform:optimization process", "perform:hyperparameter configurations", "perform:empirical evidence", "perform:bayesian", "perform:methods", "perform:lowest cross validation error", "perform:optimization", "select:result", "select:more sophisticated methods", "select:bayesian optimization", "select:date", "select:run", "select:parallel", "select:cost", "select:optimization method", "select:space", "select:trivial yet effective parallelization", "select:simple approach", "select:parallelization", "select:hyperparameter", "select:independent runs", "select:optimization process", "select:hyperparameter configurations", "select:empirical evidence", "select:bayesian", "select:methods", "select:lowest cross validation error", "select:optimization", "exist:result", "exist:more sophisticated methods", "exist:bayesian optimization", "exist:date", "exist:run", "exist:parallel", "exist:cost", "exist:optimization method", "exist:space", "exist:trivial yet effective parallelization", "exist:simple approach", "exist:parallelization", "exist:hyperparameter", "exist:independent runs", "exist:optimization process", "exist:hyperparameter configurations", "exist:empirical evidence", "exist:bayesian", "exist:methods", "exist:lowest cross validation error", "exist:optimization", "be:result", "be:more sophisticated methods", "be:bayesian optimization", "be:date", "be:run", "be:parallel", "be:cost", "be:optimization method", "be:space", "be:trivial yet effective parallelization", "be:simple approach", "be:parallelization", "be:hyperparameter", "be:independent runs", "be:optimization process", "be:hyperparameter configurations", "be:empirical evidence", "be:bayesian", "be:methods", "be:lowest cross validation error", "be:optimization", "outperform:result", "outperform:more sophisticated methods", "outperform:bayesian optimization", "outperform:date", "outperform:run", "outperform:parallel", "outperform:cost", "outperform:optimization method", "outperform:space", "outperform:trivial yet effective parallelization", "outperform:simple approach", "outperform:parallelization", "outperform:hyperparameter", "outperform:independent runs", "outperform:optimization process", "outperform:hyperparameter configurations", "outperform:empirical evidence", "outperform:bayesian", "outperform:methods", "outperform:lowest cross validation error", "outperform:optimization", "use:result", "use:more sophisticated methods", "use:bayesian optimization", "use:date", "use:run", "use:parallel", "use:cost", "use:optimization method", "use:space", "use:trivial yet effective parallelization", "use:simple approach", "use:parallelization", "use:hyperparameter", "use:independent runs", "use:optimization process", "use:hyperparameter configurations", "use:empirical evidence", "use:bayesian", "use:methods", "use:lowest cross validation error", "use:optimization", "vary:result", "vary:more sophisticated methods", "vary:bayesian optimization", "vary:date", "vary:run", "vary:parallel", "vary:cost", "vary:optimization method", "vary:space", "vary:trivial yet effective parallelization", "vary:simple approach", "vary:parallelization", "vary:hyperparameter", "vary:independent runs", "vary:optimization process", "vary:hyperparameter configurations", "vary:empirical evidence", "vary:bayesian", "vary:methods", "vary:lowest cross validation error", "vary:optimization"]]]␞effective parallelization␟optimization process␟independent runs␟optimization method␟parallel and␟sophisticated methods␟Bayesian optimization␟empirical evidence␟simple approach␟hyperparameter configurations␟trivial␟yet␟FORMULA␟select␟result␟lowest␟error␟date␟use␟cost␟evaluating␝
100047375␞61667912␟2616619952␞1604.07269␞Unfortunately, Bayesian optimization is sequential by nature: while a certain level of parallelization is easy to achieve by conditioning decisions on expectations over multiple hallucinated performance values for currently running hyperparameter evaluations CIT or by evaluating the optima of multiple acquisition functions concurrently MAINCIT , perfect parallelization appears difficult to achieve since the decisions in each step depend on all data points gathered so far.␞␞[[1, ["be:hyperparameter evaluations", "be:certain level", "be:conditioning", "be:acquisition", "be:decisions", "be:step", "be:multiple acquisition functions", "be:nature", "be:bayesian", "be:data", "be:performance", "be:data points", "be:parallelization", "be:conditioning decisions", "be:optima", "be:hyperparameter", "be:perfect parallelization", "be:bayesian optimization", "be:expectations", "be:multiple performance values", "be:hyperparameter evaluations", "be:certain level", "be:acquisition", "be:multiple acquisition functions", "be:performance", "be:conditioning decisions", "be:parallelization", "be:optima", "be:hyperparameter", "be:conditioning", "be:expectations", "be:multiple performance values", "MAINCIT:multiple acquisition functions", "MAINCIT:acquisition", "MAINCIT:optima", "appear:hyperparameter evaluations", "appear:certain level", "appear:acquisition", "appear:decisions", "appear:step", "appear:multiple acquisition functions", "appear:data", "appear:performance", "appear:data points", "appear:conditioning decisions", "appear:parallelization", "appear:optima", "appear:hyperparameter", "appear:perfect parallelization", "appear:conditioning", "appear:expectations", "appear:multiple performance values", "depend:data points", "depend:decisions", "depend:step", "depend:data"]]]␟[[1, ["be:hyperparameter evaluations", "be:certain level", "be:conditioning", "be:acquisition", "be:decisions", "be:step", "be:multiple acquisition functions", "be:nature", "be:bayesian", "be:data", "be:performance", "be:data points", "be:parallelization", "be:conditioning decisions", "be:optima", "be:hyperparameter", "be:perfect parallelization", "be:bayesian optimization", "be:expectations", "be:multiple performance values", "be:hyperparameter evaluations", "be:certain level", "be:conditioning", "be:acquisition", "be:decisions", "be:step", "be:multiple acquisition functions", "be:nature", "be:bayesian", "be:data", "be:performance", "be:data points", "be:parallelization", "be:conditioning decisions", "be:optima", "be:hyperparameter", "be:perfect parallelization", "be:bayesian optimization", "be:expectations", "be:multiple performance values", "MAINCIT:hyperparameter evaluations", "MAINCIT:certain level", "MAINCIT:conditioning", "MAINCIT:acquisition", "MAINCIT:decisions", "MAINCIT:step", "MAINCIT:multiple acquisition functions", "MAINCIT:nature", "MAINCIT:bayesian", "MAINCIT:data", "MAINCIT:performance", "MAINCIT:data points", "MAINCIT:parallelization", "MAINCIT:conditioning decisions", "MAINCIT:optima", "MAINCIT:hyperparameter", "MAINCIT:perfect parallelization", "MAINCIT:bayesian optimization", "MAINCIT:expectations", "MAINCIT:multiple performance values", "appear:hyperparameter evaluations", "appear:certain level", "appear:conditioning", "appear:acquisition", "appear:decisions", "appear:step", "appear:multiple acquisition functions", "appear:nature", "appear:bayesian", "appear:data", "appear:performance", "appear:data points", "appear:parallelization", "appear:conditioning decisions", "appear:optima", "appear:hyperparameter", "appear:perfect parallelization", "appear:bayesian optimization", "appear:expectations", "appear:multiple performance values", "depend:hyperparameter evaluations", "depend:certain level", "depend:conditioning", "depend:acquisition", "depend:decisions", "depend:step", "depend:multiple acquisition functions", "depend:nature", "depend:bayesian", "depend:data", "depend:performance", "depend:data points", "depend:parallelization", "depend:conditioning decisions", "depend:optima", "depend:hyperparameter", "depend:perfect parallelization", "depend:bayesian optimization", "depend:expectations", "depend:multiple performance values"]]]␞multiple acquisition functions␟Bayesian optimization␟certain level␟perfect parallelization␟data points␟Unfortunately␟sequential␟nature␟easy␟conditioning␟decisions␟expectations␟performance␟running␟evaluations␟evaluating␟concurrently␟difficult␟step␟far␝
100047375␞2146409231␟2131241448␞1503.00693␞There has been work to parallelize Bayesian optimization, making it possible to leverage the power of multicore architectures MAINCIT .␞␞[[1, ["be:power", "be:bayesian", "be:multicore architectures", "be:leverage", "be:work", "be:bayesian optimization", "parallelize:bayesian optimization", "parallelize:bayesian", "leverage:leverage", "leverage:multicore architectures", "leverage:power"]]]␟[[1, ["be:power", "be:bayesian", "be:multicore architectures", "be:leverage", "be:work", "be:bayesian optimization", "parallelize:power", "parallelize:bayesian", "parallelize:multicore architectures", "parallelize:leverage", "parallelize:work", "parallelize:bayesian optimization", "leverage:power", "leverage:bayesian", "leverage:multicore architectures", "leverage:leverage", "leverage:work", "leverage:bayesian optimization"]]]␞Bayesian optimization␟multicore architectures␟work␟parallelize␟making␟possible␟leverage␟power␝multicore architectures
100047375␞60686164␟60686164␞1609.08923␞Using a Bayesian optimization package called SMAC MAINCIT , we systematically evaluated a large space of such models, each of which makes its prediction based only on general features that can be computed from any normal form game.␞␞[[1, ["Using:bayesian", "Using:optimization", "Using:bayesian optimization package", "Using:smac", "MAINCIT:smac", "evaluate:such models", "evaluate:prediction", "evaluate:normal form game", "evaluate:form", "evaluate:smac", "evaluate:general features", "evaluate:bayesian", "evaluate:large space", "evaluate:bayesian optimization package", "evaluate:optimization", "make:normal form game", "make:prediction", "make:form", "make:general features", "compute:normal form game", "compute:form"]]]␟[[1, ["Using:such models", "Using:prediction", "Using:normal form game", "Using:form", "Using:smac", "Using:general features", "Using:bayesian", "Using:large space", "Using:bayesian optimization package", "Using:optimization", "MAINCIT:such models", "MAINCIT:prediction", "MAINCIT:normal form game", "MAINCIT:form", "MAINCIT:smac", "MAINCIT:general features", "MAINCIT:bayesian", "MAINCIT:large space", "MAINCIT:bayesian optimization package", "MAINCIT:optimization", "evaluate:such models", "evaluate:prediction", "evaluate:normal form game", "evaluate:form", "evaluate:smac", "evaluate:general features", "evaluate:bayesian", "evaluate:large space", "evaluate:bayesian optimization package", "evaluate:optimization", "make:such models", "make:prediction", "make:normal form game", "make:form", "make:smac", "make:general features", "make:bayesian", "make:large space", "make:bayesian optimization package", "make:optimization", "compute:such models", "compute:prediction", "compute:normal form game", "compute:form", "compute:smac", "compute:general features", "compute:bayesian", "compute:large space", "compute:bayesian optimization package", "compute:optimization"]]]␞normal form game␟Bayesian optimization␟optimization package␟large space␟such models␟general features␟called␟SMAC␟systematically␟prediction␟computed␝
100047375␞60686164␟60686164␞1609.08923␞Bayesian Optimization We performed Bayesian optimization using SMAC MAINCIT , a software package for optimizing the configuration of algorithms.␞2777904410␞[[1, ["perform:software package", "perform:smac", "perform:configuration", "perform:algorithms", "perform:bayesian", "perform:bayesian optimization", "perform:software", "perform:bayesian optimization", "optimize:algorithms", "optimize:configuration"]]]␟[[1, ["perform:software package", "perform:smac", "perform:configuration", "perform:algorithms", "perform:bayesian", "perform:bayesian optimization", "perform:software", "perform:bayesian optimization", "optimize:software package", "optimize:smac", "optimize:configuration", "optimize:algorithms", "optimize:bayesian", "optimize:bayesian optimization", "optimize:software", "optimize:bayesian optimization"]]]␞Bayesian Optimization␟Bayesian optimization␟software package␟using␟SMAC␝
100047375␞␞1301.1942␞However, as shown by MAINCIT , multiple independent runs can also improve the performance of SMAC and especially ParamILS.␞␞[[1, ["improve:paramils", "improve:multiple independent runs", "improve:performance", "improve:smac"]]]␟[[1, ["improve:paramils", "improve:multiple independent runs", "improve:performance", "improve:smac"]]]␞independent runs␟multiple␟also␟performance␟SMAC␟and␟especially␟ParamILS␝
1000617247␞␞1601.04746␞For example, MAINCIT modify the Ncut optimization function by adding in the numerator the number of violated explicit constraints (independently of them being ML or CL), times a parameter FORMULA .␞␞[[1, ["modify:parameter", "modify:ncut", "modify:explicit constraints", "modify:example", "modify:ncut optimization function", "modify:numerator", "modify:number", "modify:ml", "modify:optimization", "modify:cl", "add:parameter", "add:explicit constraints", "add:numerator", "add:number", "add:ml", "add:cl"]]]␟[[1, ["modify:parameter", "modify:ncut", "modify:explicit constraints", "modify:example", "modify:ncut optimization function", "modify:numerator", "modify:number", "modify:ml", "modify:optimization", "modify:cl", "add:parameter", "add:ncut", "add:explicit constraints", "add:example", "add:ncut optimization function", "add:numerator", "add:number", "add:ml", "add:optimization", "add:cl"]]]␞ML or CL␟explicit constraints␟example␟Ncut␟function␟adding␟numerator␟number␟violated␟independently␟being␟times␟FORMULA␝
1000617247␞␞1601.04746␞The formulation of MAINCIT incorporates all constraints into the data matrix, essentially by adding a signed Laplacian, which is a generalization of the Laplacian for graphs with negative weights; notably, their algorithm does not solve a spectral relaxation of the problem but attempts to solve the (hard) optimization problem exactly, via a continuous optimization approach.␞11413529␟11413529␟137836250␞[[1, ["incorporate:formulation", "incorporate:generalization", "incorporate:constraints", "incorporate:negative weights", "incorporate:algorithm", "incorporate:laplacian", "incorporate:optimization problem", "incorporate:continuous optimization approach", "incorporate:data", "incorporate:problem", "incorporate:graphs", "incorporate:spectral relaxation", "incorporate:data matrix", "incorporate:optimization", "add:laplacian", "add:generalization", "add:negative weights", "add:graphs", "be:laplacian", "be:generalization", "be:negative weights", "be:graphs", "solve:algorithm", "solve:optimization problem", "solve:continuous optimization approach", "solve:problem", "solve:spectral relaxation", "solve:optimization", "attempt:optimization", "attempt:optimization problem"]]]␟[[1, ["incorporate:formulation", "incorporate:generalization", "incorporate:constraints", "incorporate:negative weights", "incorporate:algorithm", "incorporate:laplacian", "incorporate:optimization problem", "incorporate:continuous optimization approach", "incorporate:data", "incorporate:problem", "incorporate:graphs", "incorporate:spectral relaxation", "incorporate:data matrix", "incorporate:optimization", "add:formulation", "add:generalization", "add:constraints", "add:negative weights", "add:algorithm", "add:laplacian", "add:optimization problem", "add:continuous optimization approach", "add:data", "add:problem", "add:graphs", "add:spectral relaxation", "add:data matrix", "add:optimization", "be:formulation", "be:generalization", "be:constraints", "be:negative weights", "be:algorithm", "be:laplacian", "be:optimization problem", "be:continuous optimization approach", "be:data", "be:problem", "be:graphs", "be:spectral relaxation", "be:data matrix", "be:optimization", "solve:formulation", "solve:generalization", "solve:constraints", "solve:negative weights", "solve:algorithm", "solve:laplacian", "solve:optimization problem", "solve:continuous optimization approach", "solve:data", "solve:problem", "solve:graphs", "solve:spectral relaxation", "solve:data matrix", "solve:optimization", "attempt:formulation", "attempt:generalization", "attempt:constraints", "attempt:negative weights", "attempt:algorithm", "attempt:laplacian", "attempt:optimization problem", "attempt:continuous optimization approach", "attempt:data", "attempt:problem", "attempt:graphs", "attempt:spectral relaxation", "attempt:data matrix", "attempt:optimization"]]]␞hard optimization problem␟continuous optimization approach␟data matrix␟signed Laplacian␟negative weights␟problem but␟formulation␟essentially␟adding␟generalization␟notably␟algorithm␟spectral␟exactly␝
1000617247␞␞1601.04746␞The formulation in MAINCIT also leads to a fast implementation of its spectral relaxation.␞␞[[1, ["lead:spectral relaxation", "lead:formulation", "lead:fast implementation"]]]␟[[1, ["lead:spectral relaxation", "lead:formulation", "lead:fast implementation"]]]␞formulation␟also␟spectral␝
